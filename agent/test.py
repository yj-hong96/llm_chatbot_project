import os
import re
import asyncio
from dotenv import load_dotenv
from groq import Groq, RateLimitError
from typing import TypedDict, List
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from pymilvus import connections, Collection
from langgraph.graph import StateGraph, END

# --- 1. ì´ˆê¸° ì„¤ì • (ì´ì „ê³¼ ë™ì¼) ---
load_dotenv()
try:
    groq_client = Groq()
except Exception as e:
    print(f"ì˜¤ë¥˜: Groq í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {e}")
    exit()

MILVUS_HOST = "localhost"
MILVUS_PORT = "19530"
COLLECTION_NAME = "farmer"
EMBEDDING_MODEL = "jhgan/ko-sroberta-multitask"
LLM_TEMPERATURE = 0.7

print("ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

print("Milvusì— ì—°ê²°í•˜ê³  ì»¬ë ‰ì…˜ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
try:
    connections.connect("default", host=MILVUS_HOST, port=MILVUS_PORT)
    farmer_collection = Collection(COLLECTION_NAME)
    farmer_collection.load()
    print("Milvus ì»¬ë ‰ì…˜ ë¡œë“œ ì™„ë£Œ.")
except Exception as e:
    print(f"ì˜¤ë¥˜: Milvus ì»¬ë ‰ì…˜ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {e}")
    exit()

# --- 2. LangGraph RAG ì—ì´ì „íŠ¸ ì •ì˜ (ë‹¨ì¼ ì§ˆë¬¸ ì²˜ë¦¬ìš©) ---
class AgentState(TypedDict):
    messages: List[BaseMessage]
    documents: List[Document]

def retrieve_documents(state: AgentState) -> AgentState:
    print(f"--- Retriever ì‹¤í–‰ (ì§ˆë¬¸: '{state['messages'][-1].content[:30]}...') ---")
    last_message = state['messages'][-1]
    query_vector = embeddings.embed_query(last_message.content)
    search_params = {"metric_type": "L2", "params": {"nprobe": 10}}
    results = farmer_collection.search(data=[query_vector], anns_field="vector", param=search_params, limit=3, output_fields=["text"])
    retrieved_docs = [Document(page_content=hit.entity.get('text')) for hit in results[0]] if results and results[0] else []
    return {"documents": retrieved_docs}

def generate_response(state: AgentState) -> AgentState:
    print(f"--- Generator ì‹¤í–‰ ---")
    context = "\n\n".join([doc.page_content for doc in state['documents']])
    system_prompt = "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ [ì°¸ê³  ì •ë³´]ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ë†ì—… ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •ë³´ê°€ ì—†ìœ¼ë©´ 'ì œê³µëœ ì •ë³´ì—ëŠ” í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”. ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤."
    user_prompt = f"[ì°¸ê³  ì •ë³´]\n{context}\n\n[ì§ˆë¬¸]\n{state['messages'][-1].content}"
    api_messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
    chat_completion = groq_client.chat.completions.create(messages=api_messages, model="llama-3.3-70b-versatile", temperature=LLM_TEMPERATURE)
    bot_response_content = chat_completion.choices[0].message.content
    return {"messages": [AIMessage(content=bot_response_content)]}

def clean_markdown(text: str) -> str:
    """LLMì´ ìƒì„±í•œ ë§ˆí¬ë‹¤ìš´ ì„œì‹ì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜"""
    text = re.sub(r'#+\s*', '', text)
    text = re.sub(r'^\s*[\*\-]\s*|\s*\d+\.\s*', '', text, flags=re.MULTILINE)
    text = re.sub(r'\*\*', '', text)
    return text.strip()

workflow = StateGraph(AgentState)
workflow.add_node("retriever", retrieve_documents)
workflow.add_node("generator", generate_response)
workflow.set_entry_point("retriever")
workflow.add_edge("retriever", "generator")
workflow.add_edge("generator", END)
rag_app = workflow.compile()


# --- 3. ì§ˆë¬¸ ë¶„í•´ ë° ì¢…í•©ì„ ìœ„í•œ í•¨ìˆ˜ë“¤ ---

def decompose_query(user_query: str, history: List[BaseMessage]) -> List[str]:
    """ì‚¬ìš©ìì˜ ë³µì¡í•œ ì§ˆë¬¸ì„ ê²€ìƒ‰ì— ìš©ì´í•œ ì—¬ëŸ¬ ê°œì˜ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•©ë‹ˆë‹¤."""
    print("\n--- Decomposer ë…¸ë“œ ì‹¤í–‰ ---")
    
    history_str = "\n".join([f"{'ì‚¬ìš©ì' if isinstance(msg, HumanMessage) else 'ì±—ë´‡'}: {msg.content}" for msg in history])

    decomposer_prompt = f"""ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìµœì‹  ì§ˆë¬¸ì„ ëª…í™•í•˜ê³  ê²€ìƒ‰ ê°€ëŠ¥í•œ í•˜ìœ„ ì§ˆë¬¸ë“¤ë¡œ ë¶„í•´í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ëª©í‘œëŠ” ì˜¤ì§ 'ê²€ìƒ‰'ì— ê°€ì¥ íš¨ìœ¨ì ì¸ í˜•íƒœë¡œ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

[ì´ì „ ëŒ€í™” ê¸°ë¡]
{history_str}

[ì ˆëŒ€ ê·œì¹™]
- **ì ˆëŒ€ ì´ì „ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ê±°ë‚˜ ì •ë¦¬í•˜ì§€ ë§ˆì„¸ìš”.** ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì˜¤ì§ ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸ì„ ë¶„í•´í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
- ê° í•˜ìœ„ ì§ˆë¬¸ì€ ë…ë¦½ì ìœ¼ë¡œ ê²€ìƒ‰ë  ìˆ˜ ìˆë„ë¡ ì™„ì „í•œ ë¬¸ì¥ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.
- ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬, 'ê·¸ê±°', 'ì–´ë–»ê²Œ', 'ì™œ' ì™€ ê°™ì€ ëª¨í˜¸í•œ í‘œí˜„ì´ ì–´ë–¤ êµ¬ì²´ì ì¸ ëŒ€ìƒ(ì˜ˆ: ì…€ëŸ¬ë¦¬)ì„ ì§€ì¹­í•˜ëŠ”ì§€ ëª…í™•íˆ í•˜ì—¬ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•˜ì„¸ìš”.
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì´ë¯¸ ë‹¨ìˆœí•˜ê³  ëª…í™•í•˜ë‹¤ë©´, ë¶ˆí•„ìš”í•˜ê²Œ ë‚˜ëˆ„ì§€ ë§ê³  ê±°ì˜ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
- ìµœì¢… ì¶œë ¥ì€ ì˜¤ì§ ë¶„í•´ëœ ì§ˆë¬¸ ëª©ë¡ì´ì–´ì•¼ í•˜ë©°, ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ì œëª©ì„ ì ˆëŒ€ í¬í•¨í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.
- ì¶œë ¥í•˜ëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ëŠ” ìˆœìˆ˜ í•œê¸€ì´ì–´ì•¼ í•©ë‹ˆë‹¤. (ì˜ì–´, í•œì ë“± ê¸ˆì§€)

[ë¶„í•´ ì˜ˆì‹œ 1]
- ì´ì „ ëŒ€í™”: ê³ ë­ì§€ ì‘ë¬¼ë¡œ ì…€ëŸ¬ë¦¬ë¥¼ ì¶”ì²œí•¨.
- ì‚¬ìš©ì ì§ˆë¬¸: ê·¸ëŸ¼ ì–´ë–»ê²Œ ì¬ë°°í•˜ê³  ìˆ˜í™•ì€ ì–¸ì œ í•´?
- ì¶œë ¥:
ì…€ëŸ¬ë¦¬ ì¬ë°° ë°©ë²•
ì…€ëŸ¬ë¦¬ ìˆ˜í™• ì‹œê¸°

[ë¶„í•´ ì˜ˆì‹œ 2]
- ì´ì „ ëŒ€í™”: ì—†ìŒ
- ì‚¬ìš©ì ì§ˆë¬¸: ë°°ì¶”ì˜ ë³‘í•´ì¶© ì¢…ë¥˜ì™€ ë°©ì œë²• ì•Œë ¤ì¤˜.
- ì¶œë ¥:
ë°°ì¶”ì˜ ì£¼ìš” ë³‘í•´ì¶© ì¢…ë¥˜
ë°°ì¶” ë³‘í•´ì¶© ë°©ì œë²•

[ì‹¤ì œ ë¶„í•´ ì‘ì—…]
- ì‚¬ìš©ì ì§ˆë¬¸: {user_query}
- ì¶œë ¥:"""
    chat_completion = groq_client.chat.completions.create(messages=[{"role": "user", "content": decomposer_prompt}], model="llama-3.1-8b-instant", temperature=0.0)
    decomposed_queries = chat_completion.choices[0].message.content.strip().split('\n')
    
    if not decomposed_queries or all(q.strip() == '' for q in decomposed_queries):
        decomposed_queries = [user_query]
        print(f"ì§ˆë¬¸ ë¶„í•´ ì‹¤íŒ¨. ì›ë³¸ ì§ˆë¬¸ ì‚¬ìš©: {decomposed_queries}")
    else:
        decomposed_queries = [q.strip() for q in decomposed_queries if q.strip()]
        print(f"ë¶„í•´ëœ ì§ˆë¬¸: {decomposed_queries}")
        
    return decomposed_queries

def synthesize_results(original_query: str, intermediate_answers: List[dict], history: List[BaseMessage]) -> str:
    """ê° í•˜ìœ„ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ë“¤ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    print("\n--- Synthesizer ë…¸ë“œ ì‹¤í–‰ ---")
    
    history_str = "\n".join([f"{'ì‚¬ìš©ì' if isinstance(msg, HumanMessage) else 'ì±—ë´‡'}: {msg.content}" for msg in history])
    
    context = ""
    for item in intermediate_answers:
        context += f"### í•˜ìœ„ ì§ˆë¬¸: {item['sub_query']}\në‹µë³€: {item['answer']}\n\n"
        
    synthesizer_prompt = f"""ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ìœ ëŠ¥í•œ 'ë†ì—… ê¸°ìˆ  ì „ë¬¸ AI ì¡°ìˆ˜'ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì„ë¬´ë¥¼ ë§¡ì•˜ìŠµë‹ˆë‹¤.

[ì´ì „ ëŒ€í™” ê¸°ë¡]
{history_str}

[ìƒˆë¡œ ê²€ìƒ‰ëœ ì •ë³´]
{context}

[ì›ë˜ ì§ˆë¬¸]
{original_query}

[ë‹µë³€ ìƒì„± ì§€ì¹¨]
1.  **í˜ë¥´ì†Œë‚˜ ìœ ì§€**: í•­ìƒ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì¡°ìˆ˜ì˜ ë§íˆ¬ë¥¼ ìœ ì§€í•˜ì„¸ìš”. ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ëª…í™•í•˜ê³  ë¶€ë“œëŸ¬ìš´ ëŒ€í™”ì²´ë¡œ ë‹µë³€ì„ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
2.  **ì •ë³´ ì¢…í•© ë° ì¬êµ¬ì„±**: [ìƒˆë¡œ ê²€ìƒ‰ëœ ì •ë³´]ì— ìˆëŠ” ê°ê°ì˜ ë‹µë³€ë“¤ì„ ë‹¨ìˆœíˆ ë‚˜ì—´í•˜ì§€ ë§ˆì„¸ìš”. ëª¨ë“  ì •ë³´ë¥¼ ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°í•˜ê³ , ë‚´ìš©ì´ ì¤‘ë³µëœë‹¤ë©´ í•˜ë‚˜ë¡œ ìš”ì•½í•˜ì—¬ í•˜ë‚˜ì˜ ì™„ì„±ëœ ë‹µë³€ìœ¼ë¡œ ì¬êµ¬ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
3.  **ì‘ë¬¼ ì´ë¦„ ì¼ë°˜í™” (ë§¤ìš° ì¤‘ìš”)**: ë‹µë³€ì— 'ìœ íƒ€ê°œëŸ‰ 15í˜¸', 'ì„¤í–¥', 'ëŒ€ê´€ë ¹'ê³¼ ê°™ì€ êµ¬ì²´ì ì¸ **í’ˆì¢…** ì´ë¦„ì´ ì–¸ê¸‰ë  ê²½ìš°, ë°˜ë“œì‹œ ê·¸ê²ƒì´ ì†í•œ **ìƒìœ„ ì‘ë¬¼**(ì˜ˆ: ì…€ëŸ¬ë¦¬, ë”¸ê¸°, ê°ì) ì´ë¦„ìœ¼ë¡œ ì¼ë°˜í™”í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”. ì‚¬ìš©ìëŠ” í’ˆì¢…ì´ ì•„ë‹Œ ì‘ë¬¼ ìì²´ì— ëŒ€í•´ ê¶ê¸ˆí•´í•©ë‹ˆë‹¤.
4.  **ì–¸ì–´ ìˆœìˆ˜ì„±**: ìµœì¢… ë‹µë³€ì€ **ì˜¤ì§ ìˆœìˆ˜ í•œê¸€**ë¡œë§Œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì˜ì–´, í•œì(ì˜ˆ: é€²è¡Œ->ì§„í–‰), ì¼ë³¸ì–´, ì´ëª¨í‹°ì½˜, ê¹¨ì§„ ë¬¸ì ë“± ë‹¤ë¥¸ ì–¸ì–´ë‚˜ ë¬¸ìëŠ” ì ˆëŒ€ í¬í•¨í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.
5.  **í˜•ì‹ ì—„ìˆ˜**: ì œëª©(##), ëª©ë¡(*, 1.), êµµì€ ê¸€ì”¨(**) ë“± ì–´ë–¤ ì¢…ë¥˜ì˜ ë§ˆí¬ë‹¤ìš´ ì„œì‹ë„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ì˜¤ì§ ìˆœìˆ˜í•œ ë¬¸ì¥ìœ¼ë¡œë§Œ ë‹µë³€ì„ êµ¬ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
6.  **ì†”ì§í•¨ê³¼ ê·¼ê±° ê¸°ë°˜ ë‹µë³€**: ë‹µë³€ì€ ë°˜ë“œì‹œ [ìƒˆë¡œ ê²€ìƒ‰ëœ ì •ë³´]ì— ìˆëŠ” ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤. ë§Œì•½ ì •ë³´ê°€ ì§ˆë¬¸ì— ë‹µí•˜ê¸°ì— ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ë‹¤ë©´, ì ˆëŒ€ë¡œ ì¶”ì¸¡í•˜ê±°ë‚˜ ê¾¸ë©°ë‚´ì§€ ë§ˆì„¸ìš”. ì´ ê²½ìš°, "ì£„ì†¡í•˜ì§€ë§Œ, ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” í•´ë‹¹ ë‚´ìš©ì— ëŒ€í•´ ì •í™•íˆ ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."ë¼ê³  ì†”ì§í•˜ê²Œ ë§í•´ì•¼ í•©ë‹ˆë‹¤.
7.  **[ì¶”ê°€] í›„ì† ì§ˆë¬¸ ìœ ë„**: ëª¨ë“  ë‹µë³€ì´ ëë‚œ í›„, ë§ˆì§€ë§‰ì— ì‚¬ìš©ìê°€ ê¶ê¸ˆí•´í•  ë§Œí•œ ê´€ë ¨ ì§ˆë¬¸ì„ í•œë‘ ê°€ì§€ ì œì•ˆí•˜ì—¬ ëŒ€í™”ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ìœ ë„í•˜ì„¸ìš”. ì§ˆë¬¸ ì•ì—ëŠ” ë¬¼ìŒí‘œ ì´ëª¨ì§€(â“)ë¥¼ ë¶™ì—¬ì£¼ì„¸ìš”.

ìœ„ ì§€ì¹¨ì— ë”°ë¼ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.

[ìµœì¢… ë‹µë³€ ì˜ˆì‹œ]
(ì‚¬ìš©ìê°€ 'ì…€ëŸ¬ë¦¬ ì¬ë°°ë²•'ì„ ë¬¼ì–´ë´¤ì„ ê²½ìš°ì˜ ë‹µë³€ ì˜ˆì‹œì…ë‹ˆë‹¤)

ì…€ëŸ¬ë¦¬ëŠ” ì„œëŠ˜í•œ ê¸°í›„ë¥¼ ì¢‹ì•„í•˜ëŠ” ì‘ë¬¼ì´ë¼ ì˜¨ë„ ê´€ë¦¬ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤. ë³´í†µ 15ë„ì—ì„œ 20ë„ ì‚¬ì´ë¥¼ ìœ ì§€í•´ì£¼ëŠ” ê²ƒì´ ì¢‹ê³ , í™ì´ ë§ˆë¥´ì§€ ì•Šë„ë¡ ë¬¼ì„ ì¶©ë¶„íˆ ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤. ë„ˆë¬´ ê±´ì¡°í•˜ë©´ ì¤„ê¸°ê°€ ë”±ë”±í•´ì ¸ í’ˆì§ˆì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

â“ ì…€ëŸ¬ë¦¬ì˜ ë³‘í•´ì¶© ì˜ˆë°© ë°©ë²•ì— ëŒ€í•´ ë” ì•Œì•„ë³¼ê¹Œìš”?
â“ ì…€ëŸ¬ë¦¬ë¥¼ ìˆ˜í™•í•œ í›„ ì–´ë–»ê²Œ ë³´ê´€í•˜ëŠ” ê²ƒì´ ì¢‹ì€ì§€ ì•Œë ¤ë“œë¦´ê¹Œìš”?

[ìµœì¢… ë‹µë³€]"""
    chat_completion = groq_client.chat.completions.create(messages=[{"role": "user", "content": synthesizer_prompt}], model="llama-3.1-8b-instant", temperature=LLM_TEMPERATURE)
    final_answer = chat_completion.choices[0].message.content
    print("ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ.")
    return final_answer

# --- 4. ë©”ì¸ ë¡œì§ (ì§ˆë¬¸ ë¶„í•´ íŒŒì´í”„ë¼ì¸) ---
async def main():
    """ì§ˆë¬¸ ë¶„í•´ ê¸°ë°˜ì˜ ì±—ë´‡ ë©”ì¸ í•¨ìˆ˜"""
    print("ì•ˆë…•í•˜ì„¸ìš© ì‘ë¬¼ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš© ì§ˆë¬¸ í•´ì£¼ì„¸ìš©^^")
    print("-" * 70)

    conversation_history: List[BaseMessage] = []

    while True:
        user_input = input("ë‚˜: ")
        if user_input.lower() == 'ì¢…ë£Œ':
            print("ì±—ë´‡: ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        sub_queries = decompose_query(user_input, conversation_history)

        intermediate_answers = []
        print("\n--- ê° í•˜ìœ„ ì§ˆë¬¸ì— ëŒ€í•œ RAG ì‹¤í–‰ ì‹œì‘ ---")
        
        rate_limit_reached = False
        for sub_query in sub_queries:
            try:
                rag_result = rag_app.invoke({"messages": [HumanMessage(content=sub_query)]})
                answer = rag_result['messages'][-1].content
                intermediate_answers.append({"sub_query": sub_query, "answer": answer})
            
            except RateLimitError:
                print("\n" + "="*70)
                print("ğŸš« API ì‚¬ìš©ëŸ‰ ì´ˆê³¼ ì•Œë¦¼ ğŸš«".center(68))
                print("="*70)
                print("í˜„ì¬ Groq APIì˜ í•˜ë£¨ ì‚¬ìš© ê°€ëŠ¥ëŸ‰ì„ ëª¨ë‘ ì†Œì§„í–ˆìŠµë‹ˆë‹¤.")
                print("ì´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ë” ì´ìƒ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("\n[í•´ê²° ë°©ë²•]")
                print("- ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì‹œê±°ë‚˜, ë‚´ì¼ API ì‚¬ìš©ëŸ‰ì´ ì´ˆê¸°í™”ëœ í›„ ì´ìš©í•´ ì£¼ì„¸ìš”.")
                print("-" * 70)
                rate_limit_reached = True
                break
        
        if rate_limit_reached:
            continue
            
        final_answer = synthesize_results(user_input, intermediate_answers, conversation_history)
        
        conversation_history.append(HumanMessage(content=user_input))
        conversation_history.append(AIMessage(content=final_answer))

        cleaned_answer = clean_markdown(final_answer)

        print("\n" + "="*70)
        print(" ìµœì¢… ë‹µë³€ ".center(70, "="))
        print("="*70)
        print(f"ì±—ë´‡: {final_answer}")
        print("-" * 70)

if __name__ == "__main__":
    # ê·¸ë˜í”„ ì‹œê°í™”
    try:
        graph_image_path = "agent_workflow.png"
        with open(graph_image_path, "wb") as f:
            # ë³€ìˆ˜ ì´ë¦„ì„ 'rag_app'ìœ¼ë¡œ ìˆ˜ì •í•˜ì—¬ ì˜¤ë¥˜ í•´ê²°
            f.write(rag_app.get_graph().draw_mermaid_png())
        print(f"\nâœ… LangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n[ì•Œë¦¼] ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    # ì´ë¯¸ì§€ ìƒì„± í›„, ì±—ë´‡ì˜ ë©”ì¸ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    asyncio.run(main())
