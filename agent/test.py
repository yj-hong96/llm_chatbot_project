import os
import re
import asyncio
from dotenv import load_dotenv
from groq import Groq, RateLimitError
from typing import TypedDict, List
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from pymilvus import connections, Collection
from langgraph.graph import StateGraph, END

# --- 1. ì´ˆê¸° ì„¤ì • (ì´ì „ê³¼ ë™ì¼) ---
load_dotenv()
try:
    groq_client = Groq()
except Exception as e:
    print(f"ì˜¤ë¥˜: Groq í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {e}")
    exit()

MILVUS_HOST = "localhost"
MILVUS_PORT = "19530"
COLLECTION_NAME = "farmer"
EMBEDDING_MODEL = "jhgan/ko-sroberta-multitask"
LLM_TEMPERATURE = 0.7

print("ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

print("Milvusì— ì—°ê²°í•˜ê³  ì»¬ë ‰ì…˜ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
try:
    connections.connect("default", host=MILVUS_HOST, port=MILVUS_PORT)
    farmer_collection = Collection(COLLECTION_NAME)
    farmer_collection.load()
    print("Milvus ì»¬ë ‰ì…˜ ë¡œë“œ ì™„ë£Œ.")
except Exception as e:
    print(f"ì˜¤ë¥˜: Milvus ì»¬ë ‰ì…˜ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {e}")
    exit()

# ====[ìˆ˜ì •ëœ ë¶€ë¶„ 1: RAG ì—ì´ì „íŠ¸ ì¬ì„¤ê³„]====
# ê¸°ì¡´ì˜ ë³µì¡í•œ 'ì§ˆë¬¸ ë¶„í•´ -> ë³‘ë ¬ RAG -> ë‹µë³€ ì¢…í•©' êµ¬ì¡°ëŠ” API í˜¸ì¶œì´ ë„ˆë¬´ ë§ì•„ ë¹„íš¨ìœ¨ì ì´ì—ˆìŠµë‹ˆë‹¤.
# ì´ì œ 'DB ê²€ìƒ‰ -> ë‹µë³€ ìƒì„±'ì˜ ë‹¨ì¼ RAG íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë‹¨ìˆœí™”í•˜ì—¬ API í˜¸ì¶œì„ ë‹¨ 1íšŒë¡œ ìµœì†Œí™”í•©ë‹ˆë‹¤.

# --- 2. LangGraph ìƒíƒœ ë° ë…¸ë“œ ì •ì˜ (ë‹¨ìˆœí™”ëœ êµ¬ì¡°) ---

# LangGraph ì—ì´ì „íŠ¸ì˜ ìƒíƒœë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
class AgentState(TypedDict):
    messages: List[BaseMessage]
    documents: List[Document]

# [ë…¸ë“œ 1: Retriever] Milvusì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜
def retrieve_documents(state: AgentState) -> AgentState:
    """ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ Milvusì—ì„œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    print(f"\n--- Retriever ì‹¤í–‰ (ì§ˆë¬¸: '{state['messages'][-1].content[:30]}...') ---")
    last_message = state['messages'][-1]
    query_vector = embeddings.embed_query(last_message.content)
    
    search_params = {"metric_type": "L2", "params": {"nprobe": 10}}
    results = farmer_collection.search(
        data=[query_vector], 
        anns_field="vector", 
        param=search_params, 
        limit=5, # ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ 5ê°œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        output_fields=["text", "source", "page"]
    )
    
    retrieved_docs = [
        Document(
            page_content=hit.entity.get('text'), 
            metadata={"source": hit.entity.get('source'), "page": hit.entity.get('page')}
        ) for hit in results[0]
    ] if results and results[0] else []
    
    print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ {len(retrieved_docs)}ê°œ")
    return {"documents": retrieved_docs}

# [ë…¸ë“œ 2: Generator] ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ëŒ€í™” ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
def generate_final_response(state: AgentState) -> AgentState:
    """
    ëª¨ë“  ê²€ìƒ‰ëœ ì •ë³´ì™€ ëŒ€í™” ê¸°ë¡ì„ ì¢…í•©í•˜ì—¬, ë‹¨ í•œ ë²ˆì˜ API í˜¸ì¶œë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ê¸°ì¡´ì˜ Synthesizer ì—­í• ì„ ì´ í•¨ìˆ˜ê°€ ëª¨ë‘ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    print("--- Generator ì‹¤í–‰ (ìµœì¢… ë‹µë³€ ìƒì„±) ---")
    messages = state['messages']
    documents = state['documents']

    context = "\n\n".join([f"[ì¶œì²˜: {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')}, {doc.metadata.get('page', 'N/A')}í˜ì´ì§€]\n{doc.page_content}" for doc in documents])
    history_str = "\n".join([f"{'ì‚¬ìš©ì' if isinstance(msg, HumanMessage) else 'ì±—ë´‡'}: {msg.content}" for msg in messages[:-1]]) # ë§ˆì§€ë§‰ ì‚¬ìš©ì ì§ˆë¬¸ ì œì™¸

    # ìµœì¢… ë‹µë³€ ìƒì„±ì„ ìœ„í•œ ê°•í™”ëœ í”„ë¡¬í”„íŠ¸
    final_prompt = f"""ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ìœ ëŠ¥í•œ 'ë†ì—… ê¸°ìˆ  ì „ë¬¸ AI ì¡°ìˆ˜'ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì„ë¬´ë¥¼ ë§¡ì•˜ìŠµë‹ˆë‹¤.

[ì´ì „ ëŒ€í™” ê¸°ë¡]
{history_str}

[ìƒˆë¡œ ê²€ìƒ‰ëœ ì°¸ê³  ì •ë³´]
{context}

[ì‚¬ìš©ìì˜ ìµœì‹  ì§ˆë¬¸]
{messages[-1].content}

[ë‹µë³€ ìƒì„± ì§€ì¹¨]
1.  **í’ˆì¢… ì´ë¦„ ì œê±° ë° ì‘ë¬¼ëª…ìœ¼ë¡œ ì¼ë°˜í™” (ê°€ì¥ ì¤‘ìš”í•œ ê·œì¹™)**: ë‹µë³€ ë‚´ìš©ì— 'ì„¤í–¥', 'ê¸ˆì‹¤', 'ìœ íƒ€ê°œëŸ‰' ê°™ì€ êµ¬ì²´ì ì¸ **í’ˆì¢…** ì´ë¦„ì´ ë‚˜ì˜¨ë‹¤ë©´, **ì ˆëŒ€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ë§ê³ ** ë°˜ë“œì‹œ ëŒ€í‘œ **ì‘ë¬¼** ì´ë¦„ì¸ 'ë”¸ê¸°', 'ì…€ëŸ¬ë¦¬' ë“±ìœ¼ë¡œ ë°”ê¿”ì„œ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤. ìµœì¢… ë‹µë³€ì—ëŠ” í’ˆì¢… ì´ë¦„ì´ ë‹¨ í•˜ë‚˜ë„ í¬í•¨ë˜ì–´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.
2.  **ëª©ë¡í™” ìš°ì„ **: ì‚¬ìš©ìê°€ "ëª¨ë“  ì¢…ë¥˜", "ë‹¤ì–‘í•˜ê²Œ" ë“± ëª©ë¡ì„ ìš”ì²­í•˜ëŠ” ê²½ìš°, [ìƒˆë¡œ ê²€ìƒ‰ëœ ì°¸ê³  ì •ë³´]ì— ì–¸ê¸‰ëœ ëª¨ë“  ê³ ìœ í•œ ì‘ë¬¼ ì´ë¦„ì„ ë¨¼ì € ë‚˜ì—´í•œ í›„, ê° ì‘ë¬¼ì— ëŒ€í•œ ì„¤ëª…ì„ ìš”ì•½í•˜ì—¬ ì œê³µí•˜ì„¸ìš”.
3.  **ì •ë³´ ì¢…í•©**: ë‚´ìš©ì„ ë‹¨ìˆœíˆ ë‚˜ì—´í•˜ì§€ ë§ê³ , ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°í•˜ê³  ì¤‘ë³µì„ ì œê±°í•˜ì—¬ í•˜ë‚˜ì˜ ì™„ì„±ëœ ê¸€ë¡œ ì¬êµ¬ì„±í•˜ì„¸ìš”.
4.  **ì–¸ì–´ ë° í˜•ì‹**: ë‹µë³€ì€ **ì˜¤ì§ ìˆœìˆ˜ í•œê¸€**ë¡œë§Œ ì‘ì„±ë˜ì–´ì•¼ í•˜ë©°, ì–´ë–¤ ì¢…ë¥˜ì˜ ë§ˆí¬ë‹¤ìš´ ì„œì‹(ì œëª©, ëª©ë¡, êµµì€ ê¸€ì”¨ ë“±)ë„ ì ˆëŒ€ ì‚¬ìš©í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.
5.  **ê·¼ê±° ê¸°ë°˜ ë‹µë³€**: ë‹µë³€ì€ ë°˜ë“œì‹œ [ìƒˆë¡œ ê²€ìƒ‰ëœ ì°¸ê³  ì •ë³´]ì— ìˆëŠ” ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤. ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ "ì£„ì†¡í•˜ì§€ë§Œ, ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” í•´ë‹¹ ë‚´ìš©ì— ëŒ€í•´ ì •í™•íˆ ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."ë¼ê³  ì†”ì§í•˜ê²Œ ë§í•´ì•¼ í•©ë‹ˆë‹¤.
6.  **ì§€ëŠ¥ì ì¸ í›„ì† ì§ˆë¬¸**: ë‹µë³€ì˜ í•µì‹¬ ì£¼ì œì™€ ê´€ë ¨ëœ í›„ì† ì§ˆë¬¸ì„ í•œë‘ ê°€ì§€ ì œì•ˆí•˜ì„¸ìš”. ì§ˆë¬¸ ì•ì—ëŠ” ë¬¼ìŒí‘œ ì´ëª¨ì§€(â“)ë¥¼ ë¶™ì—¬ì£¼ì„¸ìš”.

ìœ„ ì§€ì¹¨ì— ë”°ë¼ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.

[ìµœì¢… ë‹µë³€]"""
    
    api_messages = [{"role": "user", "content": final_prompt}]
    
    # ë‹¨ì¼ API í˜¸ì¶œë¡œ ìµœì¢… ë‹µë³€ ìƒì„±
    chat_completion = groq_client.chat.completions.create(
        messages=api_messages, 
        model="llama-3.3-70b-versatile", # ë‹µë³€ í’ˆì§ˆì„ ìœ„í•´ ê³ ì„±ëŠ¥ ëª¨ë¸ ì‚¬ìš©
        temperature=LLM_TEMPERATURE
    )
    final_answer = chat_completion.choices[0].message.content
    print("ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ.")
    
    return {"messages": [AIMessage(content=final_answer)]}

# --- 3. LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì¶• (ë‹¨ìˆœí™”ëœ êµ¬ì¡°) ---
workflow = StateGraph(AgentState)
workflow.add_node("retriever", retrieve_documents)
workflow.add_node("generator", generate_final_response) # ë‹µë³€ ìƒì„± í•¨ìˆ˜ ë³€ê²½
workflow.set_entry_point("retriever")
workflow.add_edge("retriever", "generator")
workflow.add_edge("generator", END)
rag_app = workflow.compile()


# --- 4. ì±—ë´‡ ë©”ì¸ ë¡œì§ (ë‹¨ìˆœí™”ëœ êµ¬ì¡°) ---
def main():
    """ë‹¨ìˆœí™”ëœ RAG ê¸°ë°˜ ì±—ë´‡ì˜ ë©”ì¸ í•¨ìˆ˜"""
    print("ì•ˆë…•í•˜ì„¸ìš© ì‘ë¬¼ì— í•´ë‹¹í•˜ëŠ” ë‚´ìš© ì§ˆë¬¸ í•´ì£¼ì„¸ìš©^^")
    print("-" * 70)

    # ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ (ë‹¨ê¸° ê¸°ì–µ)
    conversation_history: List[BaseMessage] = []

    while True:
        user_input = input("ë‚˜: ")
        if user_input.lower() == 'ì¢…ë£Œ':
            print("ì±—ë´‡: ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # ====[ìˆ˜ì •ëœ ë¶€ë¶„ 2: ë©”ì¸ ë¡œì§ ë‹¨ìˆœí™”]====
        # decompose_queryì™€ synthesize_results í•¨ìˆ˜ í˜¸ì¶œì„ ì œê±°í•˜ê³ ,
        # ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë°”ë¡œ RAG ì•±ì— ì „ë‹¬í•˜ì—¬ ë‹¨ì¼ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        # í˜„ì¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì „ì²´ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        current_messages = conversation_history + [HumanMessage(content=user_input)]
        
        try:
            # LangGraph ì—ì´ì „íŠ¸ ì‹¤í–‰ (API í˜¸ì¶œ 1íšŒ ë°œìƒ)
            final_state = rag_app.invoke({"messages": current_messages})
            
            # ì—ì´ì „íŠ¸ì˜ ìµœì¢… ì‘ë‹µì„ ê°€ì ¸ì˜´
            bot_response_message = final_state['messages'][-1]
            
            # ëŒ€í™” ê¸°ë¡ì— ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì±—ë´‡ ë‹µë³€ì„ ëª¨ë‘ ì €ì¥
            conversation_history.append(HumanMessage(content=user_input))
            conversation_history.append(bot_response_message)

            print("\n" + "="*70)
            print(" ìµœì¢… ë‹µë³€ ".center(70, "="))
            print("="*70)
            print(f"ì±—ë´‡: {bot_response_message.content}")
            print("-" * 70)

        except RateLimitError:
            print("\n" + "="*70)
            print("ğŸš« API ì‚¬ìš©ëŸ‰ ì´ˆê³¼ ì•Œë¦¼ ğŸš«".center(68))
            print("="*70)
            print("í˜„ì¬ Groq APIì˜ í•˜ë£¨ ì‚¬ìš© ê°€ëŠ¥ëŸ‰ì„ ëª¨ë‘ ì†Œì§„í–ˆìŠµë‹ˆë‹¤.")
            print("\n[í•´ê²° ë°©ë²•]")
            print("- ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì‹œê±°ë‚˜, ë‚´ì¼ API ì‚¬ìš©ëŸ‰ì´ ì´ˆê¸°í™”ëœ í›„ ì´ìš©í•´ ì£¼ì„¸ìš”.")
            print("-" * 70)
        except Exception as e:
            print(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

if __name__ == "__main__":
    # ê·¸ë˜í”„ ì‹œê°í™”
    try:
        graph_image_path = "agent_workflow.png"
        with open(graph_image_path, "wb") as f:
            f.write(rag_app.get_graph().draw_mermaid_png())
        print(f"\nâœ… LangGraph êµ¬ì¡°ê°€ '{graph_image_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n[ì•Œë¦¼] ê·¸ë˜í”„ ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    # ì´ë¯¸ì§€ ìƒì„± í›„, ì±—ë´‡ì˜ ë©”ì¸ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    # ë” ì´ìƒ ë³µì¡í•œ ë¹„ë™ê¸° ì²˜ë¦¬ê°€ í•„ìš” ì—†ìœ¼ë¯€ë¡œ asyncio.run()ì„ ì œê±°í•˜ê³  ì§ì ‘ main()ì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
    main()

